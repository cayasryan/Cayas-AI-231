{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B Evaluation\n",
    "Ryan Roi Cayas\\\n",
    "2022-22085\n",
    "\n",
    "In this notebook, we replicate the published scores of the Llama 3.2 1B on the MGSM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load libraries and set-up CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets # Huggingface datasets (https://huggingface.co/docs/datasets/)\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Set-up CUDA device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# use a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Use GPU for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check the GPU name\n",
    "if device.type == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)  # 0 because CUDA_VISIBLE_DEVICES=4 means GPU 4 is now 0\n",
    "    print(\"Using GPU:\", gpu_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load the pre-trained tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths to model and tokenizer\n",
    "model_dir = \"../../../../../llm/llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir, padding_side=\"left\")\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Set the eos_token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load the MGSM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 11/11 [00:39<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'answer_number', 'equation_solution', 'language'],\n",
      "    num_rows: 88\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer_number', 'language'],\n",
      "    num_rows: 2750\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Languages: English, Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, Telugu\n",
    "languages = [\"en\",\"es\",\"fr\",\"de\",\"ru\",\"zh\",\"ja\",\"th\",\"sw\",\"bn\",\"te\"]\n",
    "\n",
    "# Empty list to store datasets with added 'language' feature\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# Load the datasets and add the 'language' feature\n",
    "for lang in tqdm(languages, desc=\"Loading datasets\"):\n",
    "    # Load train and test datasets for the language\n",
    "    train = load_dataset(\"juletxara/mgsm\", lang, split=\"train\")\n",
    "    test = load_dataset(\"juletxara/mgsm\", lang, split=\"test\")\n",
    "    \n",
    "    # Add the 'language' feature to both train and test sets\n",
    "    train = train.add_column(\"language\", [lang] * len(train))\n",
    "    test = test.add_column(\"language\", [lang] * len(test))\n",
    "    \n",
    "    # Append the datasets with the 'language' feature to the lists\n",
    "    train_datasets.append(train)\n",
    "    test_datasets.append(test)\n",
    "\n",
    "# Concatenate datasets from all languages into a single train and test dataset\n",
    "mgsm_train = concatenate_datasets(train_datasets)\n",
    "\n",
    "mgsm_test = concatenate_datasets(test_datasets)\n",
    "mgsm_test = mgsm_test.remove_columns([\"answer\", \"equation_solution\"])\n",
    "\n",
    "# Verify the structure\n",
    "print(mgsm_train)\n",
    "print(mgsm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>equation_solution</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>Step-by-Step Answer: Roger started with 5 ball...</td>\n",
       "      <td>11</td>\n",
       "      <td>5 + 6 = 11.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: There were nine computers in the ser...</td>\n",
       "      <td>Step-by-Step Answer: There are 4 days from mon...</td>\n",
       "      <td>29</td>\n",
       "      <td>4 * 5 = 20. 9 + 20 = 29.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Leah had 32 chocolates and her siste...</td>\n",
       "      <td>Step-by-Step Answer: Leah had 32 chocolates an...</td>\n",
       "      <td>39</td>\n",
       "      <td>32 + 42 = 74. 74 - 35 = 39.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Shawn has five toys. For Christmas, ...</td>\n",
       "      <td>Step-by-Step Answer: He has 5 toys. He got 2 f...</td>\n",
       "      <td>9</td>\n",
       "      <td>5 + 2 = 7. 7 + 2 = 9.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Michael had 58 golf balls. On tuesda...</td>\n",
       "      <td>Step-by-Step Answer: Michael started with 58 g...</td>\n",
       "      <td>33</td>\n",
       "      <td>58 - 23 = 35. 35 - 2 = 33.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Question: Roger has 5 tennis balls. He buys 2 ...   \n",
       "1  Question: There were nine computers in the ser...   \n",
       "2  Question: Leah had 32 chocolates and her siste...   \n",
       "3  Question: Shawn has five toys. For Christmas, ...   \n",
       "4  Question: Michael had 58 golf balls. On tuesda...   \n",
       "\n",
       "                                              answer  answer_number  \\\n",
       "0  Step-by-Step Answer: Roger started with 5 ball...             11   \n",
       "1  Step-by-Step Answer: There are 4 days from mon...             29   \n",
       "2  Step-by-Step Answer: Leah had 32 chocolates an...             39   \n",
       "3  Step-by-Step Answer: He has 5 toys. He got 2 f...              9   \n",
       "4  Step-by-Step Answer: Michael started with 58 g...             33   \n",
       "\n",
       "             equation_solution language  \n",
       "0                  5 + 6 = 11.       en  \n",
       "1     4 * 5 = 20. 9 + 20 = 29.       en  \n",
       "2  32 + 42 = 74. 74 - 35 = 39.       en  \n",
       "3        5 + 2 = 7. 7 + 2 = 9.       en  \n",
       "4   58 - 23 = 35. 35 - 2 = 33.       en  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgsm_train.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>equation_solution</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Janet’s ducks lay 16 eggs per day. She eats th...</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A robe takes 2 bolts of blue fiber and half th...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Josh decides to try flipping a house.  He buys...</td>\n",
       "      <td>None</td>\n",
       "      <td>70000</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James decides to run 3 sprints 3 times a week....</td>\n",
       "      <td>None</td>\n",
       "      <td>540</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every day, Wendi feeds each of her chickens th...</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer  answer_number  \\\n",
       "0  Janet’s ducks lay 16 eggs per day. She eats th...   None             18   \n",
       "1  A robe takes 2 bolts of blue fiber and half th...   None              3   \n",
       "2  Josh decides to try flipping a house.  He buys...   None          70000   \n",
       "3  James decides to run 3 sprints 3 times a week....   None            540   \n",
       "4  Every day, Wendi feeds each of her chickens th...   None             20   \n",
       "\n",
       "  equation_solution language  \n",
       "0              None       en  \n",
       "1              None       en  \n",
       "2              None       en  \n",
       "3              None       en  \n",
       "4              None       en  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgsm_test.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Problem:\n",
      "Question: Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Answer: Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "Answer Number: 11\n",
      "Equation_Solution: 5 + 6 = 11.\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "print(\"Sample Problem:\")\n",
    "print(\"Question:\", mgsm_train[sample_idx]['question'])\n",
    "print(\"Answer:\", mgsm_train[sample_idx]['answer'])\n",
    "print(\"Answer Number:\", mgsm_train[sample_idx]['answer_number'])\n",
    "print(\"Equation_Solution:\", mgsm_train[sample_idx]['equation_solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load evaluation data from Meta\n",
    "\n",
    "The evaluation data is publicly released by Meta and is available here: https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n",
    "\n",
    "We find here the following:\n",
    "- evaluation config\n",
    "- user prompts added in every language\n",
    "- parsed answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /data/students/ryan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Load token from config.json\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "hf_token = config[\"hf_token\"]\n",
    "login(hf_token)\n",
    "eval_dataset_FROM_META = load_dataset(\"meta-llama/Llama-3.2-1B-Instruct-evals\", \"Llama-3.2-1B-Instruct-evals__mgsm__details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_name</th>\n",
       "      <th>subtask_name</th>\n",
       "      <th>input_question</th>\n",
       "      <th>input_choice_list</th>\n",
       "      <th>input_final_prompts</th>\n",
       "      <th>input_correct_responses</th>\n",
       "      <th>output_prediction_text</th>\n",
       "      <th>output_parsed_answer</th>\n",
       "      <th>output_choice_completions</th>\n",
       "      <th>output_choice_negative_log_likelihoods</th>\n",
       "      <th>output_metrics</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>input_question_hash</th>\n",
       "      <th>input_final_prompts_hash</th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>eval_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>te</td>\n",
       "      <td>ఆండ్రూ న్యూజెర్సీ నుంచి రోచెస్టర్‌కు ఒక రోడ్డు...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nఈ...</td>\n",
       "      <td>[9, 9., 9.0, 9.0., 9.00, 9.00., 9, 9., 9.0, 9....</td>\n",
       "      <td>[రోచెస్టర్ నుంచి బస్సు ద్వారా ప్రయాణించడానికి ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>cd94ca91efc39a41e4c4c4bc0c05402edf876bb9e3b947...</td>\n",
       "      <td>[c08c78b2914e6099ee463c3e4f593bf5017c36c1ed754...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>কার্লের প্রিয় খাবার হল চিজ্‌। তিনি এই সপ্তাহে ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[31, 31., 31.0, 31.0., 31.00, 31.00., 31, 31.,...</td>\n",
       "      <td>[সপ্তাহে তিনি 2*7=&lt;&lt;2*7=14&gt;&gt;14টি স্যান্ডউইচ খে...</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>6704115ad46deece36245043307151bd6fd83e02930329...</td>\n",
       "      <td>[1aec05a3e1d4a893e3b0d35624fb89159fc1c32561ec8...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>জ্যানেটের কাছে 22টি সবুজ কলম ও 10টি হলুদ কলম আ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...</td>\n",
       "      <td>[জ্যানেটের কাছে সবুজ কলমের মোট সংখ্যা হল 22টি ...</td>\n",
       "      <td>360</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>c034cb29e28be89863d8c81696515eb46614beb50893ae...</td>\n",
       "      <td>[b5dc4fc9106d844acf8f278cb90244e5f226aa9043a2a...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>ব্রিনলি মিঃ বার্টের অঙ্কের ক্লাসে আছে। প্রত্যে...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...</td>\n",
       "      <td>[প্রথম পাঁচটি পরীক্ষার স্কোর যোগ করলে আমরা পাই...</td>\n",
       "      <td>353</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>0c7da335161864651f040a8c291e0e78071a221800dcd4...</td>\n",
       "      <td>[42d35a0e59b2dce67fa4f109447b0752cd28c92d5e570...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>মাইকেল বাইক চালাতে ভালোবাসেন। তিনি এটি এক সপ্ত...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[860, 860., 860.0, 860.0., 860.00, 860.00., 86...</td>\n",
       "      <td>[মাইকেল 5 বার বাইক চালান, তাই তিনি 5*25=&lt;&lt;5*25...</td>\n",
       "      <td>860</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 1.0, 'em_maj1@1': 1.0, 'f1': 1.0, 'f1_m...</td>\n",
       "      <td>True</td>\n",
       "      <td>f11366127c3e5d1b93ddb37334135fca18618c47e5b47e...</td>\n",
       "      <td>[0540c4c88eef5b44aaa61d9152e58f02465331eca313c...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_type  task_name subtask_name  \\\n",
       "0  Generative  mgsm_chat           te   \n",
       "1  Generative  mgsm_chat           bn   \n",
       "2  Generative  mgsm_chat           bn   \n",
       "3  Generative  mgsm_chat           bn   \n",
       "4  Generative  mgsm_chat           bn   \n",
       "\n",
       "                                      input_question input_choice_list  \\\n",
       "0  ఆండ్రూ న్యూజెర్సీ నుంచి రోచెస్టర్‌కు ఒక రోడ్డు...              None   \n",
       "1  কার্লের প্রিয় খাবার হল চিজ্‌। তিনি এই সপ্তাহে ...              None   \n",
       "2  জ্যানেটের কাছে 22টি সবুজ কলম ও 10টি হলুদ কলম আ...              None   \n",
       "3  ব্রিনলি মিঃ বার্টের অঙ্কের ক্লাসে আছে। প্রত্যে...              None   \n",
       "4  মাইকেল বাইক চালাতে ভালোবাসেন। তিনি এটি এক সপ্ত...              None   \n",
       "\n",
       "                                 input_final_prompts  \\\n",
       "0  [<|start_header_id|>user<|end_header_id|>\\n\\nఈ...   \n",
       "1  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "2  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "3  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "4  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "\n",
       "                             input_correct_responses  \\\n",
       "0  [9, 9., 9.0, 9.0., 9.00, 9.00., 9, 9., 9.0, 9....   \n",
       "1  [31, 31., 31.0, 31.0., 31.00, 31.00., 31, 31.,...   \n",
       "2  [98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...   \n",
       "3  [98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...   \n",
       "4  [860, 860., 860.0, 860.0., 860.00, 860.00., 86...   \n",
       "\n",
       "                              output_prediction_text output_parsed_answer  \\\n",
       "0  [రోచెస్టర్ నుంచి బస్సు ద్వారా ప్రయాణించడానికి ...                  4.5   \n",
       "1  [সপ্তাহে তিনি 2*7=<<2*7=14>>14টি স্যান্ডউইচ খে...                   25   \n",
       "2  [জ্যানেটের কাছে সবুজ কলমের মোট সংখ্যা হল 22টি ...                  360   \n",
       "3  [প্রথম পাঁচটি পরীক্ষার স্কোর যোগ করলে আমরা পাই...                  353   \n",
       "4  [মাইকেল 5 বার বাইক চালান, তাই তিনি 5*25=<<5*25...                  860   \n",
       "\n",
       "  output_choice_completions output_choice_negative_log_likelihoods  \\\n",
       "0                      None                                   None   \n",
       "1                      None                                   None   \n",
       "2                      None                                   None   \n",
       "3                      None                                   None   \n",
       "4                      None                                   None   \n",
       "\n",
       "                                      output_metrics  is_correct  \\\n",
       "0  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "1  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "2  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "3  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "4  {'em': 1.0, 'em_maj1@1': 1.0, 'f1': 1.0, 'f1_m...        True   \n",
       "\n",
       "                                 input_question_hash  \\\n",
       "0  cd94ca91efc39a41e4c4c4bc0c05402edf876bb9e3b947...   \n",
       "1  6704115ad46deece36245043307151bd6fd83e02930329...   \n",
       "2  c034cb29e28be89863d8c81696515eb46614beb50893ae...   \n",
       "3  0c7da335161864651f040a8c291e0e78071a221800dcd4...   \n",
       "4  f11366127c3e5d1b93ddb37334135fca18618c47e5b47e...   \n",
       "\n",
       "                            input_final_prompts_hash benchmark_label  \\\n",
       "0  [c08c78b2914e6099ee463c3e4f593bf5017c36c1ed754...            MGSM   \n",
       "1  [1aec05a3e1d4a893e3b0d35624fb89159fc1c32561ec8...            MGSM   \n",
       "2  [b5dc4fc9106d844acf8f278cb90244e5f226aa9043a2a...            MGSM   \n",
       "3  [42d35a0e59b2dce67fa4f109447b0752cd28c92d5e570...            MGSM   \n",
       "4  [0540c4c88eef5b44aaa61d9152e58f02465331eca313c...            MGSM   \n",
       "\n",
       "                                         eval_config  \n",
       "0  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "1  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "2  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "3  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "4  {'max_gen_len': '2048', 'max_prompt_len': '614...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_FROM_META_df = eval_dataset_FROM_META['latest'].to_pandas()\n",
    "eval_dataset_FROM_META_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_gen_len': '2048',\n",
       " 'max_prompt_len': '6144',\n",
       " 'num_few_shot': '0',\n",
       " 'num_generations': '1',\n",
       " 'prompt_fn': \"functools.partial(<function jinja_dialog_format at 0x7f0d7e4c0d30>, template={'prompt': '{{ native_prompt }}\\\\n\\\\n{{ input }}', 'answer': '{{ target }}'}, append_gen_prefix=False)\",\n",
       " 'return_logprobs': 'false',\n",
       " 'seed': '42',\n",
       " 'temperature': '0.0',\n",
       " 'top_k': '0',\n",
       " 'top_p': '0'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check META's eval_config\n",
    "eval_dataset_FROM_META_df['eval_config'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = {\n",
    "    'max_gen_len': 2048,\n",
    "    'max_prompt_len': 6144,\n",
    "    'num_few_shot': 0,\n",
    "    'num_generations': 1,\n",
    "    # 'prompt_fn': functools.partial(jinja_dialog_format, template={'prompt': '{{ native_prompt }}\\\\n\\\\n{{ input }}', 'answer': '{{ target }}'}, append_gen_prefix=False),\n",
    "    'return_logprobs': False,\n",
    "    'seed': 42,\n",
    "    'temperature': 0.0,\n",
    "    'top_k': 0,\n",
    "    'top_p': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(eval_config['seed'])\n",
    "random.seed(eval_config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtask_name\n",
       "te    250\n",
       "bn    250\n",
       "de    250\n",
       "th    250\n",
       "en    250\n",
       "zh    250\n",
       "fr    250\n",
       "ja    250\n",
       "ru    250\n",
       "es    250\n",
       "sw    250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_FROM_META_df[\"subtask_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Identify user prompts per language\n",
    "\n",
    "We add the user prompts used by Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".\n",
      "\n",
      "Tracy used a piece of wire 4 feet long to support tomato plants in the garden. The wire was cut into pieces 6 inches long. How many pieces did she obtain?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "']\n"
     ]
    }
   ],
   "source": [
    "# check input_final_prompt for subtask_name = en\n",
    "filter = eval_dataset_FROM_META_df[\"subtask_name\"] == 'en'\n",
    "input_prompt = eval_dataset_FROM_META_df[filter]['input_final_prompts'].iloc[0]\n",
    "input_prompt = r\"{}\".format(input_prompt).replace(\"\\\\n\", \"\\n\") # convert prompt to raw string\n",
    "\n",
    "print(input_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".\n",
      "\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_header = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "user_prompts = {\n",
    "    \"en\": 'Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".',\n",
    "    \"te\": 'ఈ గణిత సమస్యను పరిష్కరించండి. చివరి సమాధానాన్ని ఇవ్వదానికి ముందు తర్కాత్మక అదుగులను ఇవ్వండి. చివరి పంక్తిలో మాత్రమే \"సమాధానం:\" అనే ఆకారంలో చివరి సమాధానాద్ని ఇవ్వండి సమాధానం: తర్వాత పూర్ణాంక సమాధానానికి తప్పించి ఎదేనా చేర్చవద్దు.',\n",
    "    \"bn\": 'এই গণিতের সমস্যাটি সমাধান করুন। চূড়ান্ত উত্তর দেওয়ার আগে যুক্তিসম্পন্ন পদক্ষেপ প্রদান করুন। চূড়ান্ত উত্তরটি একক সংখ্যা হিসাবে \"উত্তর:\" এর পরে শেষ লাইনে দিন। \"উত্তর:\" এর পরে অন্য কিছু যুক্ত করবেন না।.',\n",
    "    \"de\": 'Löse dieses Mathematikproblem. Gib die Schritte zur Begründung an, bevor du die endgültige Antwort in der letzten Zeile alleine im Format \"Antwort:\" gibst. Füge nichts anderes als die ganzzahlige Antwort nach \"Antwort:\" hinzu.',\n",
    "    \"th\": 'แก้ปัญหาคณิตศาสตร์นี้ ให้ให้ขั้นตอนการใช้เหตุผลก่อนที่จะให้คำตอบสุดท้ายในบรรทัดสุดท้ายโดยอยู่ในรูปแบบ \"คำตอบ:\" ไม่ควรเพิ่มอะไรนอกจากคำตอบที่เป็นจำนวนเต็มหลังจาก \"คำตอบ:\"',\n",
    "    \"zh\": '解决这个数学问题。在最后一行给出答案前，请提供推理步骤。最后一行应该以 \"答案: \" 的形式独立给出答案。在 \"答案：\" 后不要添加除整数答案之外的任何内容。',\n",
    "    \"fr\": 'Résolvez ce problème de mathématiques. Donnez les étapes de raisonnement avant de fournir la réponse finale sur la dernière ligne elle-même dans le format de \"Réponse:\". N\\'ajoutez rien d\\'autre que la réponse entière après \"Réponse:\".',\n",
    "    \"ja\": 'の数学の問題を解いてください。最終的な答えを出す前に、解答の推論過程を記述してください。そして最後の行には \"答え:\" の形式で答えを記述し、その後には整数の答え以外何も追加しないでください。',\n",
    "    \"ru\": 'Решите эту математическую задачу. Объясните шаги рассуждения перед тем, как дать окончательный ответ в последней строке сам по себе в формате \"Ответ:\". Не добавляйте ничего, кроме целочисленного ответа после \"Ответ:\".',\n",
    "    \"es\": 'Resuelve este problema matemático. Proporciona los pasos de razonamiento antes de dar la respuesta final en la última línea por sí misma en el formato de \"Respuesta:\". No añadas nada más que la respuesta entera después de \"Respuesta:\".',\n",
    "    \"sw\": 'Suluhisha tatizo hili la hesabu. Toa hatua za mantiki kabla ya kutoa jibu la mwisho kwenye mstari wa mwisho peke yake katika muundo wa \"Jibu:\". Usiongeze chochote kingine isipokuwa jibu la integer baada ya \"Jibu:\".'    \n",
    "}\n",
    "\n",
    "answer_translations = {\n",
    "    \"en\": \"Answer:\",\n",
    "    \"te\": \"సమాధానం:\",\n",
    "    \"bn\": \"উত্তর:\",\n",
    "    \"de\": \"Antwort:\",\n",
    "    \"th\": \"คำตอบ:\",\n",
    "    \"zh\": \"答案: \",\n",
    "    \"fr\": \"Réponse:\",\n",
    "    \"ja\": \"答え:\",\n",
    "    \"ru\": \"Ответ:\",\n",
    "    \"es\": \"Respuesta:\",\n",
    "    \"sw\": \"Jibu:\"\n",
    "}\n",
    "\n",
    "# sample prompt for an English question\n",
    "sample_idx = 0\n",
    "question = mgsm_test[sample_idx]['question']\n",
    "language = mgsm_test[sample_idx]['language']\n",
    "sample_prompt = user_header + \"\\n\\n\" + user_prompts[language] + \"\\n\\n\" + question + \"<|eot_id|>\" + assistant_header + \"\\n\\n\"\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined in the cell above: user_header, assistant_header, user_prompts\n",
    "def add_input_prompt(question: str, language: str) -> str:\n",
    "    \"\"\"\"\n",
    "    question_features contains: \n",
    "        question\t\n",
    "        answer\t\n",
    "        answer_number\t\n",
    "        equation_solution\t\n",
    "        language\n",
    "    \"\"\"\n",
    "\n",
    "    return user_header + \"\\n\\n\" + user_prompts[language] + \"\\n\\n\" + question + \"<|eot_id|>\" + assistant_header + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create parser for the model's final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Gemma's GSM8k Evaluation Code: https://github.com/google-deepmind/gemma/blob/main/colabs/gsm8k_eval.ipynb\n",
    "\n",
    "def find_numbers(x: str) -> list[str]:\n",
    "  \"\"\"Finds all numbers in a string.\"\"\"\n",
    "  # Search for number, possibly negative (hyphen), with thousand separators\n",
    "  # (comma), and with a decimal point (period inbetween digits).\n",
    "  numbers = re.compile(\n",
    "      r'-?[\\d,]*\\.?\\d+',\n",
    "      re.MULTILINE | re.DOTALL | re.IGNORECASE,\n",
    "  ).findall(x)\n",
    "  return numbers\n",
    "\n",
    "\n",
    "def find_number(x: str,\n",
    "                answer_delimiter: str = 'Answer:') -> str:\n",
    "  \"\"\"Finds the most relevant number in a string.\"\"\"\n",
    "  # If model uses the answer delimiter, then select the first number following\n",
    "  # that format.\n",
    "  if answer_delimiter in x:\n",
    "    answer = x.split(answer_delimiter)[-1]\n",
    "    numbers = find_numbers(answer)\n",
    "    # print(\"Found delimiter!\")\n",
    "    # print(numbers)\n",
    "\n",
    "    if numbers:\n",
    "      return numbers[0]\n",
    "\n",
    "  # In general, select the last number in the string.\n",
    "  numbers = find_numbers(x)\n",
    "  # print(numbers)\n",
    "\n",
    "  if numbers:\n",
    "    return numbers[-1]\n",
    "  return ''\n",
    "\n",
    "\n",
    "def maybe_remove_comma(x: str) -> str:\n",
    "  # Example: 5,600 -> 5600\n",
    "  return x.replace(',', '')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = {\n",
    "    'max_new_tokens': eval_config['max_gen_len'],\n",
    "    'num_return_sequences': eval_config['num_generations'],\n",
    "    'output_scores': eval_config['return_logprobs'],\n",
    "    'pad_token_id': tokenizer.eos_token_id,\n",
    "    'do_sample': False,  # since temperature, top_k, top_p are 0\n",
    "    'top_p': None,\n",
    "    'temperature': None,\n",
    "    'top_k': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the profit, first calculate the increased value of the house after repairs:\n",
      "\n",
      "$80,000 + ($50,000 x 1.5) = $80,000 + $75,000 = $155,000\n",
      "\n",
      "Then, subtract the original purchase price from the increased value to find the profit:\n",
      "\n",
      "$155,000 - $80,000 = $75,000\n",
      "\n",
      "Answer: $75,000\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample inference\n",
    "sample_idx = 2\n",
    "\n",
    "input_prompt = add_input_prompt(mgsm_test[sample_idx]['question'], mgsm_test[sample_idx]['language'])\n",
    "input_tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "output = model.generate(**input_tokenized, \n",
    "                        **generate_config\n",
    "                        )\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# splice output_text after \"assistant\"\n",
    "assistant_idx = output_text.find(\"assistant\\n\\n\")\n",
    "output_text = output_text[assistant_idx + len(\"assistant\\n\\n\"):]\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Answer: 75000\n"
     ]
    }
   ],
   "source": [
    "answer_delimiter = answer_translations[mgsm_test[sample_idx]['language']]\n",
    "output_numerical_answer = maybe_remove_comma(find_number(output_text, answer_delimiter))\n",
    "\n",
    "print(\"Numerical Answer:\", output_numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [2:28:59<00:00, 81.27s/it] \n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Parameters for batch processing\n",
    "loader_config = {'batch_size': 25,\n",
    "                 'num_workers': 4, \n",
    "                 'prefetch_factor': 2,\n",
    "                 'shuffle': False}\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "test_data_loader = DataLoader(mgsm_test, **loader_config)\n",
    "\n",
    "# Initializa a dictionary to store the results\n",
    "results_mgsm = {'question': mgsm_test['question'], \n",
    "                'language': mgsm_test['language'], \n",
    "                'ground_truth': mgsm_test['answer_number'], \n",
    "                'generated_answer': []}\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_samples in tqdm(test_data_loader, total=len(test_data_loader)):\n",
    "\n",
    "    # Add input prompt to the batch problems\n",
    "    batch_problems = [add_input_prompt(question, language) for question, language in zip(batch_samples['question'], batch_samples['language'])]\n",
    "    \n",
    "    # Tokenize the input problems\n",
    "    inputs = tokenizer(batch_problems, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate outputs from the model for the entire batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generate_config)\n",
    "\n",
    "    # Get numerical answer from output texts\n",
    "    output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    numerical_answers = [maybe_remove_comma(find_number(output_text, answer_translations[language])) for output_text, language in zip(output_texts, batch_samples['language'])]\n",
    "\n",
    "    # Add the numerical answers to the results dictionary\n",
    "    results_mgsm['generated_answer'].extend(numerical_answers)\n",
    "\n",
    "# Create a directory to store the results\n",
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the results to a file\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(globals()[results_var_name], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate score\n",
    "\n",
    "We calculate the average exact match (maj@1) scores over all eleven languages in the MGSM dataset. \\\n",
    "Reference: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in generated answers: 2\n"
     ]
    }
   ],
   "source": [
    "# Check null values in generated answers\n",
    "null_values = results_mgsm['generated_answer'].count('')\n",
    "print(f\"Number of null values in generated answers: {null_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta's Overall Accuracy: 0.24472727272727274\n",
      "Personal Overall Accuracy: 0.18763636363636363\n",
      "\n",
      "\n",
      "   Language  Meta Score  Personal Score  Language Supported\n",
      "0        bn       0.144           0.144               False\n",
      "1        de       0.272           0.240                True\n",
      "2        en       0.420           0.408                True\n",
      "3        es       0.352           0.248                True\n",
      "4        fr       0.264           0.212                True\n",
      "5        ja       0.180           0.148               False\n",
      "6        ru       0.272           0.244               False\n",
      "7        sw       0.184           0.020               False\n",
      "8        te       0.112           0.016               False\n",
      "9        th       0.228           0.204                True\n",
      "10       zh       0.264           0.180               False\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm\"\n",
    "\n",
    "# Load the results from the json file and convert to pandas DataFrame\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "results_df = pd.read_json(results_file)\n",
    "\n",
    "# Convert ground_truth and generated_answer columns to float\n",
    "results_df['ground_truth'] = results_df['ground_truth'].astype(float)\n",
    "# results_df['generated_answer'] = results_df['generated_answer'].astype(float)\n",
    "results_df['generated_answer'] = pd.to_numeric(results_df['generated_answer'], errors='coerce')\n",
    "    # coerce will convert the invalid parsing to NaN\n",
    "\n",
    "\n",
    "# Compare the generated answers with the ground truth answers\n",
    "results_df['correct'] = results_df['ground_truth'] == results_df['generated_answer']\n",
    "\n",
    "# Get accuracy per language then get average accuracy\n",
    "accuracy_per_language = results_df.groupby('language')['correct'].mean()\n",
    "overall_accuracy = accuracy_per_language.mean()\n",
    "\n",
    "# Get Meta's accuracy\n",
    "meta_accuracy = eval_dataset_FROM_META_df.groupby('subtask_name')['is_correct'].mean()\n",
    "meta_accuracy_overall = meta_accuracy.mean()\n",
    "print(\"Meta's Overall Accuracy:\", meta_accuracy_overall)\n",
    "print(\"Personal Overall Accuracy:\", overall_accuracy)\n",
    "\n",
    "\n",
    "# Check if the language is supported (see https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/main/README.md?code=true)\n",
    "lang_supported = ['en', 'de', 'fr', 'it', 'pt', 'hi', 'es', 'th']\n",
    "is_supported = [lang in lang_supported for lang in accuracy_per_language.index]\n",
    "\n",
    "# Compare Meta and personal evaluation results\n",
    "eval_scores_df = pd.DataFrame({\n",
    "    'Language': meta_accuracy.index,\n",
    "    'Meta Score': meta_accuracy.values,\n",
    "    'Personal Score': accuracy_per_language.values,\n",
    "    'Language Supported': is_supported\n",
    "})\n",
    "\n",
    "print('')\n",
    "print(eval_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXTENSION: Fine-tune model on MGSM training data\n",
    "\n",
    "https://github.com/huggingface/huggingface-llama-recipes/blob/main/fine_tune/peft_finetuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preprocess train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fields(question_set):\n",
    "    language = question_set['language']\n",
    "    combined_text = f\"{question_set['question']}\\n\\n{question_set['answer']}\\n\\n{question_set['equation_solution']}\\n\\n{answer_translations[language]} {question_set['answer_number']}\"\n",
    "    question_set['text'] = combined_text\n",
    "    return question_set\n",
    "\n",
    "# Apply the function to the dataset\n",
    "mgsm_train_combined = mgsm_train.map(combine_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "\n",
      "Step-by-Step Answer: 5 bagels for $3 each should cost 5 * 3 = 15 dollars. Olivia had $23 in the beginning, so now she has 23 - 15 = 8 dollars left. The answer is 8.\n",
      "\n",
      "5 * 3 = 15. 23 - 15 = 8.\n",
      "\n",
      "Answer: 8\n"
     ]
    }
   ],
   "source": [
    "print(mgsm_train_combined['text'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tune the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/students/ryan/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcayasryan\u001b[0m (\u001b[33mcayasryan-university-of-the-philippines-diliman\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/raid/students/ryan/Cayas-AI-231/machine_exers/ME4/wandb/run-20241011_080416-6d6rslhw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface/runs/6d6rslhw' target=\"_blank\">./finetune_results</a></strong> to <a href='https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface' target=\"_blank\">https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface/runs/6d6rslhw' target=\"_blank\">https://wandb.ai/cayasryan-university-of-the-philippines-diliman/huggingface/runs/6d6rslhw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 02:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.834600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=110, training_loss=1.0022768627513539, metrics={'train_runtime': 144.5373, 'train_samples_per_second': 6.088, 'train_steps_per_second': 0.761, 'total_flos': 2485041893277696.0, 'train_loss': 1.0022768627513539, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install peft trl bitsandbytes\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import BitsAndBytesConfig, EarlyStoppingCallback\n",
    "\n",
    "# UNCOMMENT if need to unload memory\n",
    "# torch.cuda.empty_cache()\n",
    "# del trainer\n",
    "\n",
    "QLoRA = True\n",
    "if QLoRA:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=\"all-linear\",\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./finetune_results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir='./finetune_logs',\n",
    "    logging_steps=25,\n",
    "    # load_best_model_at_end = True,\n",
    "    # evaluation_strategy = \"steps\",\n",
    "    dataset_text_field=\"text\",  # Set the dataset text field here\n",
    "    max_seq_length=2048  # Set the max sequence length\n",
    ")\n",
    "\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=mgsm_train_combined,\n",
    "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 110/110 [2:51:32<00:00, 93.57s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Load fine tuned model\n",
    "# del model # delete base model to free up GPU space\n",
    "model_dir = \"./finetune_results/checkpoint-110\"\n",
    "model_finetuned = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "model_finetuned.to(device)\n",
    "\n",
    "# Parameters for batch processing\n",
    "loader_config = {'batch_size': 25,\n",
    "                 'num_workers': 4, \n",
    "                 'prefetch_factor': 2,\n",
    "                 'shuffle': False}\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "test_data_loader = DataLoader(mgsm_test, **loader_config)\n",
    "\n",
    "# Initializa a dictionary to store the results\n",
    "results_mgsm_finetuned_2 = {'question': mgsm_test['question'], \n",
    "                'language': mgsm_test['language'], \n",
    "                'ground_truth': mgsm_test['answer_number'], \n",
    "                'generated_answer': []}\n",
    "\n",
    "# Disable parallelism in tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_samples in tqdm(test_data_loader, total=len(test_data_loader)):\n",
    "\n",
    "    # Add input prompt to the batch problems\n",
    "    batch_problems = [add_input_prompt(question, language) for question, language in zip(batch_samples['question'], batch_samples['language'])]\n",
    "    \n",
    "    # Tokenize the input problems\n",
    "    inputs = tokenizer(batch_problems, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate outputs from the model for the entire batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model_finetuned.generate(**inputs, **generate_config)\n",
    "\n",
    "    # Get numerical answer from output texts\n",
    "    output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    numerical_answers = [maybe_remove_comma(find_number(output_text, answer_translations[language])) for output_text, language in zip(output_texts, batch_samples['language'])]\n",
    "\n",
    "    # Add the numerical answers to the results dictionary\n",
    "    results_mgsm_finetuned_2['generated_answer'].extend(numerical_answers)\n",
    "\n",
    "# Create a directory to store the results\n",
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm_finetuned_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the results to a file\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(globals()[results_var_name], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate updated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta's Overall Accuracy: 0.24472727272727274\n",
      "Personal Overall Accuracy: 0.1941818181818182\n",
      "\n",
      "   Language  Meta Score  Personal Score  Language Supported\n",
      "0        bn       0.144           0.156               False\n",
      "1        de       0.272           0.232                True\n",
      "2        en       0.420           0.340                True\n",
      "3        es       0.352           0.168                True\n",
      "4        fr       0.264           0.248                True\n",
      "5        ja       0.180           0.160               False\n",
      "6        ru       0.272           0.244               False\n",
      "7        sw       0.184           0.120               False\n",
      "8        te       0.112           0.048               False\n",
      "9        th       0.228           0.188                True\n",
      "10       zh       0.264           0.232               False\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm_finetuned_2\"\n",
    "\n",
    "# Load the results from the json file and convert to pandas DataFrame\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "results_df = pd.read_json(results_file)\n",
    "\n",
    "# Convert ground_truth and generated_answer columns to float\n",
    "results_df['ground_truth'] = results_df['ground_truth'].astype(float)\n",
    "# results_df['generated_answer'] = results_df['generated_answer'].astype(float)\n",
    "results_df['generated_answer'] = pd.to_numeric(results_df['generated_answer'], errors='coerce')\n",
    "    # coerce will convert the invalid parsing to NaN\n",
    "\n",
    "\n",
    "# Compare the generated answers with the ground truth answers\n",
    "results_df['correct'] = results_df['ground_truth'] == results_df['generated_answer']\n",
    "\n",
    "# Get accuracy per language then get average accuracy\n",
    "accuracy_per_language = results_df.groupby('language')['correct'].mean()\n",
    "overall_accuracy = accuracy_per_language.mean()\n",
    "\n",
    "# Get Meta's accuracy\n",
    "meta_accuracy = eval_dataset_FROM_META_df.groupby('subtask_name')['is_correct'].mean()\n",
    "meta_accuracy_overall = meta_accuracy.mean()\n",
    "print(\"Meta's Overall Accuracy:\", meta_accuracy_overall)\n",
    "print(\"Personal Overall Accuracy:\", overall_accuracy)\n",
    "\n",
    "\n",
    "# Check if the language is supported (see https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/blob/main/README.md?code=true)\n",
    "lang_supported = ['en', 'de', 'fr', 'it', 'pt', 'hi', 'es', 'th']\n",
    "is_supported = [lang in lang_supported for lang in accuracy_per_language.index]\n",
    "\n",
    "# Compare Meta and personal evaluation results\n",
    "eval_scores_df = pd.DataFrame({\n",
    "    'Language': meta_accuracy.index,\n",
    "    'Meta Score': meta_accuracy.values,\n",
    "    'Personal Score': accuracy_per_language.values,\n",
    "    'Language Supported': is_supported\n",
    "})\n",
    "\n",
    "print('')\n",
    "print(eval_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fine-tuned model weights in ./finetune_results\n"
     ]
    }
   ],
   "source": [
    "# Delete the fine-tuned model weights\n",
    "model_dir = \"./finetune_results\"\n",
    "if os.path.exists(model_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(model_dir)\n",
    "    print(f\"Deleted the fine-tuned model weights in {model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
