{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B Evaluation\n",
    "Ryan Roi Cayas\\\n",
    "2022-22085\n",
    "\n",
    "In this notebook, we replicate the published scores of the Llama 3.2 1B on the MGSM dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load libraries and set-up CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset, concatenate_datasets # Huggingface datasets (https://huggingface.co/docs/datasets/)\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "# Set-up CUDA device\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "# use a specific GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "# Use GPU for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check the GPU name\n",
    "if device.type == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)  # 0 because CUDA_VISIBLE_DEVICES=4 means GPU 4 is now 0\n",
    "    print(\"Using GPU:\", gpu_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load the pre-trained tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths to model and tokenizer\n",
    "model_dir = \"../../../../../llm/llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir, padding_side=\"left\")\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Set the eos_token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load the MGSM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 11/11 [00:32<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'answer_number', 'equation_solution', 'language'],\n",
      "    num_rows: 88\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'answer_number', 'equation_solution', 'language'],\n",
      "    num_rows: 2750\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Languages: English, Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, Telugu\n",
    "languages = [\"en\",\"es\",\"fr\",\"de\",\"ru\",\"zh\",\"ja\",\"th\",\"sw\",\"bn\",\"te\"]\n",
    "\n",
    "# Empty list to store datasets with added 'language' feature\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# Load the datasets and add the 'language' feature\n",
    "for lang in tqdm(languages, desc=\"Loading datasets\"):\n",
    "    # Load train and test datasets for the language\n",
    "    train = load_dataset(\"juletxara/mgsm\", lang, split=\"train\")\n",
    "    test = load_dataset(\"juletxara/mgsm\", lang, split=\"test\")\n",
    "    \n",
    "    # Add the 'language' feature to both train and test sets\n",
    "    train = train.add_column(\"language\", [lang] * len(train))\n",
    "    test = test.add_column(\"language\", [lang] * len(test))\n",
    "    \n",
    "    # Append the datasets with the 'language' feature to the lists\n",
    "    train_datasets.append(train)\n",
    "    test_datasets.append(test)\n",
    "\n",
    "# Concatenate datasets from all languages into a single train and test dataset\n",
    "mgsm_train = concatenate_datasets(train_datasets)\n",
    "mgsm_test = concatenate_datasets(test_datasets)\n",
    "\n",
    "# Verify the structure\n",
    "print(mgsm_train)\n",
    "print(mgsm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>equation_solution</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question: Roger has 5 tennis balls. He buys 2 ...</td>\n",
       "      <td>Step-by-Step Answer: Roger started with 5 ball...</td>\n",
       "      <td>11</td>\n",
       "      <td>5 + 6 = 11.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: There were nine computers in the ser...</td>\n",
       "      <td>Step-by-Step Answer: There are 4 days from mon...</td>\n",
       "      <td>29</td>\n",
       "      <td>4 * 5 = 20. 9 + 20 = 29.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: Leah had 32 chocolates and her siste...</td>\n",
       "      <td>Step-by-Step Answer: Leah had 32 chocolates an...</td>\n",
       "      <td>39</td>\n",
       "      <td>32 + 42 = 74. 74 - 35 = 39.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question: Shawn has five toys. For Christmas, ...</td>\n",
       "      <td>Step-by-Step Answer: He has 5 toys. He got 2 f...</td>\n",
       "      <td>9</td>\n",
       "      <td>5 + 2 = 7. 7 + 2 = 9.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Question: Michael had 58 golf balls. On tuesda...</td>\n",
       "      <td>Step-by-Step Answer: Michael started with 58 g...</td>\n",
       "      <td>33</td>\n",
       "      <td>58 - 23 = 35. 35 - 2 = 33.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Question: Roger has 5 tennis balls. He buys 2 ...   \n",
       "1  Question: There were nine computers in the ser...   \n",
       "2  Question: Leah had 32 chocolates and her siste...   \n",
       "3  Question: Shawn has five toys. For Christmas, ...   \n",
       "4  Question: Michael had 58 golf balls. On tuesda...   \n",
       "\n",
       "                                              answer  answer_number  \\\n",
       "0  Step-by-Step Answer: Roger started with 5 ball...             11   \n",
       "1  Step-by-Step Answer: There are 4 days from mon...             29   \n",
       "2  Step-by-Step Answer: Leah had 32 chocolates an...             39   \n",
       "3  Step-by-Step Answer: He has 5 toys. He got 2 f...              9   \n",
       "4  Step-by-Step Answer: Michael started with 58 g...             33   \n",
       "\n",
       "             equation_solution language  \n",
       "0                  5 + 6 = 11.       en  \n",
       "1     4 * 5 = 20. 9 + 20 = 29.       en  \n",
       "2  32 + 42 = 74. 74 - 35 = 39.       en  \n",
       "3        5 + 2 = 7. 7 + 2 = 9.       en  \n",
       "4   58 - 23 = 35. 35 - 2 = 33.       en  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgsm_train.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_number</th>\n",
       "      <th>equation_solution</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Janet’s ducks lay 16 eggs per day. She eats th...</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A robe takes 2 bolts of blue fiber and half th...</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Josh decides to try flipping a house.  He buys...</td>\n",
       "      <td>None</td>\n",
       "      <td>70000</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James decides to run 3 sprints 3 times a week....</td>\n",
       "      <td>None</td>\n",
       "      <td>540</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every day, Wendi feeds each of her chickens th...</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer  answer_number  \\\n",
       "0  Janet’s ducks lay 16 eggs per day. She eats th...   None             18   \n",
       "1  A robe takes 2 bolts of blue fiber and half th...   None              3   \n",
       "2  Josh decides to try flipping a house.  He buys...   None          70000   \n",
       "3  James decides to run 3 sprints 3 times a week....   None            540   \n",
       "4  Every day, Wendi feeds each of her chickens th...   None             20   \n",
       "\n",
       "  equation_solution language  \n",
       "0              None       en  \n",
       "1              None       en  \n",
       "2              None       en  \n",
       "3              None       en  \n",
       "4              None       en  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgsm_test.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Problem:\n",
      "Question: Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Answer: Step-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "Answer Number: 11\n",
      "Equation_Solution: 5 + 6 = 11.\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "print(\"Sample Problem:\")\n",
    "print(\"Question:\", mgsm_train[sample_idx]['question'])\n",
    "print(\"Answer:\", mgsm_train[sample_idx]['answer'])\n",
    "print(\"Answer Number:\", mgsm_train[sample_idx]['answer_number'])\n",
    "print(\"Equation_Solution:\", mgsm_train[sample_idx]['equation_solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load evaluation data from Meta\n",
    "\n",
    "The evaluation data is publicly released by Meta and is available here: https://huggingface.co/datasets/meta-llama/Llama-3.2-1B-Instruct-evals\n",
    "\n",
    "We find here the following:\n",
    "- evaluation config\n",
    "- user prompts added in every language\n",
    "- parsed answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /data/students/ryan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2594b8e56244ca897b6e4bc64f91e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_2024-09-23T17-23-09.030591.parquet.gzip:   0%|          | 0.00/1.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1a2e3638ed4971820f8eeb56b234fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating latest split:   0%|          | 0/2750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load token from config.json\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "hf_token = config[\"hf_token\"]\n",
    "login(hf_token)\n",
    "eval_dataset_FROM_META = load_dataset(\"meta-llama/Llama-3.2-1B-Instruct-evals\", \"Llama-3.2-1B-Instruct-evals__mgsm__details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_name</th>\n",
       "      <th>subtask_name</th>\n",
       "      <th>input_question</th>\n",
       "      <th>input_choice_list</th>\n",
       "      <th>input_final_prompts</th>\n",
       "      <th>input_correct_responses</th>\n",
       "      <th>output_prediction_text</th>\n",
       "      <th>output_parsed_answer</th>\n",
       "      <th>output_choice_completions</th>\n",
       "      <th>output_choice_negative_log_likelihoods</th>\n",
       "      <th>output_metrics</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>input_question_hash</th>\n",
       "      <th>input_final_prompts_hash</th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>eval_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>te</td>\n",
       "      <td>ఆండ్రూ న్యూజెర్సీ నుంచి రోచెస్టర్‌కు ఒక రోడ్డు...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nఈ...</td>\n",
       "      <td>[9, 9., 9.0, 9.0., 9.00, 9.00., 9, 9., 9.0, 9....</td>\n",
       "      <td>[రోచెస్టర్ నుంచి బస్సు ద్వారా ప్రయాణించడానికి ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>cd94ca91efc39a41e4c4c4bc0c05402edf876bb9e3b947...</td>\n",
       "      <td>[c08c78b2914e6099ee463c3e4f593bf5017c36c1ed754...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>কার্লের প্রিয় খাবার হল চিজ্‌। তিনি এই সপ্তাহে ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[31, 31., 31.0, 31.0., 31.00, 31.00., 31, 31.,...</td>\n",
       "      <td>[সপ্তাহে তিনি 2*7=&lt;&lt;2*7=14&gt;&gt;14টি স্যান্ডউইচ খে...</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>6704115ad46deece36245043307151bd6fd83e02930329...</td>\n",
       "      <td>[1aec05a3e1d4a893e3b0d35624fb89159fc1c32561ec8...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>জ্যানেটের কাছে 22টি সবুজ কলম ও 10টি হলুদ কলম আ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...</td>\n",
       "      <td>[জ্যানেটের কাছে সবুজ কলমের মোট সংখ্যা হল 22টি ...</td>\n",
       "      <td>360</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>c034cb29e28be89863d8c81696515eb46614beb50893ae...</td>\n",
       "      <td>[b5dc4fc9106d844acf8f278cb90244e5f226aa9043a2a...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>ব্রিনলি মিঃ বার্টের অঙ্কের ক্লাসে আছে। প্রত্যে...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...</td>\n",
       "      <td>[প্রথম পাঁচটি পরীক্ষার স্কোর যোগ করলে আমরা পাই...</td>\n",
       "      <td>353</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>0c7da335161864651f040a8c291e0e78071a221800dcd4...</td>\n",
       "      <td>[42d35a0e59b2dce67fa4f109447b0752cd28c92d5e570...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generative</td>\n",
       "      <td>mgsm_chat</td>\n",
       "      <td>bn</td>\n",
       "      <td>মাইকেল বাইক চালাতে ভালোবাসেন। তিনি এটি এক সপ্ত...</td>\n",
       "      <td>None</td>\n",
       "      <td>[&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nএ...</td>\n",
       "      <td>[860, 860., 860.0, 860.0., 860.00, 860.00., 86...</td>\n",
       "      <td>[মাইকেল 5 বার বাইক চালান, তাই তিনি 5*25=&lt;&lt;5*25...</td>\n",
       "      <td>860</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'em': 1.0, 'em_maj1@1': 1.0, 'f1': 1.0, 'f1_m...</td>\n",
       "      <td>True</td>\n",
       "      <td>f11366127c3e5d1b93ddb37334135fca18618c47e5b47e...</td>\n",
       "      <td>[0540c4c88eef5b44aaa61d9152e58f02465331eca313c...</td>\n",
       "      <td>MGSM</td>\n",
       "      <td>{'max_gen_len': '2048', 'max_prompt_len': '614...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_type  task_name subtask_name  \\\n",
       "0  Generative  mgsm_chat           te   \n",
       "1  Generative  mgsm_chat           bn   \n",
       "2  Generative  mgsm_chat           bn   \n",
       "3  Generative  mgsm_chat           bn   \n",
       "4  Generative  mgsm_chat           bn   \n",
       "\n",
       "                                      input_question input_choice_list  \\\n",
       "0  ఆండ్రూ న్యూజెర్సీ నుంచి రోచెస్టర్‌కు ఒక రోడ్డు...              None   \n",
       "1  কার্লের প্রিয় খাবার হল চিজ্‌। তিনি এই সপ্তাহে ...              None   \n",
       "2  জ্যানেটের কাছে 22টি সবুজ কলম ও 10টি হলুদ কলম আ...              None   \n",
       "3  ব্রিনলি মিঃ বার্টের অঙ্কের ক্লাসে আছে। প্রত্যে...              None   \n",
       "4  মাইকেল বাইক চালাতে ভালোবাসেন। তিনি এটি এক সপ্ত...              None   \n",
       "\n",
       "                                 input_final_prompts  \\\n",
       "0  [<|start_header_id|>user<|end_header_id|>\\n\\nఈ...   \n",
       "1  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "2  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "3  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "4  [<|start_header_id|>user<|end_header_id|>\\n\\nএ...   \n",
       "\n",
       "                             input_correct_responses  \\\n",
       "0  [9, 9., 9.0, 9.0., 9.00, 9.00., 9, 9., 9.0, 9....   \n",
       "1  [31, 31., 31.0, 31.0., 31.00, 31.00., 31, 31.,...   \n",
       "2  [98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...   \n",
       "3  [98, 98., 98.0, 98.0., 98.00, 98.00., 98, 98.,...   \n",
       "4  [860, 860., 860.0, 860.0., 860.00, 860.00., 86...   \n",
       "\n",
       "                              output_prediction_text output_parsed_answer  \\\n",
       "0  [రోచెస్టర్ నుంచి బస్సు ద్వారా ప్రయాణించడానికి ...                  4.5   \n",
       "1  [সপ্তাহে তিনি 2*7=<<2*7=14>>14টি স্যান্ডউইচ খে...                   25   \n",
       "2  [জ্যানেটের কাছে সবুজ কলমের মোট সংখ্যা হল 22টি ...                  360   \n",
       "3  [প্রথম পাঁচটি পরীক্ষার স্কোর যোগ করলে আমরা পাই...                  353   \n",
       "4  [মাইকেল 5 বার বাইক চালান, তাই তিনি 5*25=<<5*25...                  860   \n",
       "\n",
       "  output_choice_completions output_choice_negative_log_likelihoods  \\\n",
       "0                      None                                   None   \n",
       "1                      None                                   None   \n",
       "2                      None                                   None   \n",
       "3                      None                                   None   \n",
       "4                      None                                   None   \n",
       "\n",
       "                                      output_metrics  is_correct  \\\n",
       "0  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "1  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "2  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "3  {'em': 0.0, 'em_maj1@1': 0.0, 'f1': 0.0, 'f1_m...       False   \n",
       "4  {'em': 1.0, 'em_maj1@1': 1.0, 'f1': 1.0, 'f1_m...        True   \n",
       "\n",
       "                                 input_question_hash  \\\n",
       "0  cd94ca91efc39a41e4c4c4bc0c05402edf876bb9e3b947...   \n",
       "1  6704115ad46deece36245043307151bd6fd83e02930329...   \n",
       "2  c034cb29e28be89863d8c81696515eb46614beb50893ae...   \n",
       "3  0c7da335161864651f040a8c291e0e78071a221800dcd4...   \n",
       "4  f11366127c3e5d1b93ddb37334135fca18618c47e5b47e...   \n",
       "\n",
       "                            input_final_prompts_hash benchmark_label  \\\n",
       "0  [c08c78b2914e6099ee463c3e4f593bf5017c36c1ed754...            MGSM   \n",
       "1  [1aec05a3e1d4a893e3b0d35624fb89159fc1c32561ec8...            MGSM   \n",
       "2  [b5dc4fc9106d844acf8f278cb90244e5f226aa9043a2a...            MGSM   \n",
       "3  [42d35a0e59b2dce67fa4f109447b0752cd28c92d5e570...            MGSM   \n",
       "4  [0540c4c88eef5b44aaa61d9152e58f02465331eca313c...            MGSM   \n",
       "\n",
       "                                         eval_config  \n",
       "0  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "1  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "2  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "3  {'max_gen_len': '2048', 'max_prompt_len': '614...  \n",
       "4  {'max_gen_len': '2048', 'max_prompt_len': '614...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_FROM_META_df = eval_dataset_FROM_META['latest'].to_pandas()\n",
    "eval_dataset_FROM_META_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_gen_len': '2048',\n",
       " 'max_prompt_len': '6144',\n",
       " 'num_few_shot': '0',\n",
       " 'num_generations': '1',\n",
       " 'prompt_fn': \"functools.partial(<function jinja_dialog_format at 0x7f0d7e4c0d30>, template={'prompt': '{{ native_prompt }}\\\\n\\\\n{{ input }}', 'answer': '{{ target }}'}, append_gen_prefix=False)\",\n",
       " 'return_logprobs': 'false',\n",
       " 'seed': '42',\n",
       " 'temperature': '0.0',\n",
       " 'top_k': '0',\n",
       " 'top_p': '0'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check META's eval_config\n",
    "eval_dataset_FROM_META_df['eval_config'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = {\n",
    "    'max_gen_len': 2048,\n",
    "    'max_prompt_len': 6144,\n",
    "    'num_few_shot': 0,\n",
    "    'num_generations': 1,\n",
    "    # 'prompt_fn': functools.partial(jinja_dialog_format, template={'prompt': '{{ native_prompt }}\\\\n\\\\n{{ input }}', 'answer': '{{ target }}'}, append_gen_prefix=False),\n",
    "    'return_logprobs': False,\n",
    "    'seed': 42,\n",
    "    'temperature': 0.0,\n",
    "    'top_k': 0,\n",
    "    'top_p': 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(eval_config['seed'])\n",
    "random.seed(eval_config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtask_name\n",
       "te    250\n",
       "bn    250\n",
       "de    250\n",
       "th    250\n",
       "en    250\n",
       "zh    250\n",
       "fr    250\n",
       "ja    250\n",
       "ru    250\n",
       "es    250\n",
       "sw    250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_FROM_META_df[\"subtask_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Identify user prompts per language\n",
    "\n",
    "We add the user prompts used by Meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".\n",
      "\n",
      "Tracy used a piece of wire 4 feet long to support tomato plants in the garden. The wire was cut into pieces 6 inches long. How many pieces did she obtain?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "']\n"
     ]
    }
   ],
   "source": [
    "# check input_final_prompt for subtask_name = en\n",
    "filter = eval_dataset_FROM_META_df[\"subtask_name\"] == 'en'\n",
    "input_prompt = eval_dataset_FROM_META_df[filter]['input_final_prompts'].iloc[0]\n",
    "input_prompt = r\"{}\".format(input_prompt).replace(\"\\\\n\", \"\\n\") # convert prompt to raw string\n",
    "\n",
    "print(input_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".\n",
      "\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_header = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "user_prompts = {\n",
    "    \"en\": 'Solve this math problem. Give the reasoning steps before giving the final answer on the last line by itself in the format of \"Answer:\". Do not add anything other than the integer answer after \"Answer:\".',\n",
    "    \"te\": 'ఈ గణిత సమస్యను పరిష్కరించండి. చివరి సమాధానాన్ని ఇవ్వదానికి ముందు తర్కాత్మక అదుగులను ఇవ్వండి. చివరి పంక్తిలో మాత్రమే \"సమాధానం:\" అనే ఆకారంలో చివరి సమాధానాద్ని ఇవ్వండి సమాధానం: తర్వాత పూర్ణాంక సమాధానానికి తప్పించి ఎదేనా చేర్చవద్దు.',\n",
    "    \"bn\": 'এই গণিতের সমস্যাটি সমাধান করুন। চূড়ান্ত উত্তর দেওয়ার আগে যুক্তিসম্পন্ন পদক্ষেপ প্রদান করুন। চূড়ান্ত উত্তরটি একক সংখ্যা হিসাবে \"উত্তর:\" এর পরে শেষ লাইনে দিন। \"উত্তর:\" এর পরে অন্য কিছু যুক্ত করবেন না।.',\n",
    "    \"de\": 'Löse dieses Mathematikproblem. Gib die Schritte zur Begründung an, bevor du die endgültige Antwort in der letzten Zeile alleine im Format \"Antwort:\" gibst. Füge nichts anderes als die ganzzahlige Antwort nach \"Antwort:\" hinzu.',\n",
    "    \"th\": 'แก้ปัญหาคณิตศาสตร์นี้ ให้ให้ขั้นตอนการใช้เหตุผลก่อนที่จะให้คำตอบสุดท้ายในบรรทัดสุดท้ายโดยอยู่ในรูปแบบ \"คำตอบ:\" ไม่ควรเพิ่มอะไรนอกจากคำตอบที่เป็นจำนวนเต็มหลังจาก \"คำตอบ:\"',\n",
    "    \"zh\": '解决这个数学问题。在最后一行给出答案前，请提供推理步骤。最后一行应该以 \"答案: \" 的形式独立给出答案。在 \"答案：\" 后不要添加除整数答案之外的任何内容。',\n",
    "    \"fr\": 'Résolvez ce problème de mathématiques. Donnez les étapes de raisonnement avant de fournir la réponse finale sur la dernière ligne elle-même dans le format de \"Réponse:\". N\\'ajoutez rien d\\'autre que la réponse entière après \"Réponse:\".',\n",
    "    \"ja\": 'の数学の問題を解いてください。最終的な答えを出す前に、解答の推論過程を記述してください。そして最後の行には \"答え:\" の形式で答えを記述し、その後には整数の答え以外何も追加しないでください。',\n",
    "    \"ru\": 'Решите эту математическую задачу. Объясните шаги рассуждения перед тем, как дать окончательный ответ в последней строке сам по себе в формате \"Ответ:\". Не добавляйте ничего, кроме целочисленного ответа после \"Ответ:\".',\n",
    "    \"es\": 'Resuelve este problema matemático. Proporciona los pasos de razonamiento antes de dar la respuesta final en la última línea por sí misma en el formato de \"Respuesta:\". No añadas nada más que la respuesta entera después de \"Respuesta:\".',\n",
    "    \"sw\": 'Suluhisha tatizo hili la hesabu. Toa hatua za mantiki kabla ya kutoa jibu la mwisho kwenye mstari wa mwisho peke yake katika muundo wa \"Jibu:\". Usiongeze chochote kingine isipokuwa jibu la integer baada ya \"Jibu:\".'    \n",
    "}\n",
    "\n",
    "answer_translations = {\n",
    "    \"en\": \"Answer:\",\n",
    "    \"te\": \"సమాధానం:\",\n",
    "    \"bn\": \"উত্তর:\",\n",
    "    \"de\": \"Antwort:\",\n",
    "    \"th\": \"คำตอบ:\",\n",
    "    \"zh\": \"答案: \",\n",
    "    \"fr\": \"Réponse:\",\n",
    "    \"ja\": \"答え:\",\n",
    "    \"ru\": \"Ответ:\",\n",
    "    \"es\": \"Respuesta:\",\n",
    "    \"sw\": \"Jibu:\"\n",
    "}\n",
    "\n",
    "# sample prompt for an English question\n",
    "sample_idx = 0\n",
    "question = mgsm_test[sample_idx]['question']\n",
    "language = mgsm_test[sample_idx]['language']\n",
    "sample_prompt = user_header + \"\\n\\n\" + user_prompts[language] + \"\\n\\n\" + question + \"<|eot_id|>\" + assistant_header + \"\\n\\n\"\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined in the cell above: user_header, assistant_header, user_prompts\n",
    "def add_input_prompt(question: str, language: str) -> str:\n",
    "    \"\"\"\"\n",
    "    question_features contains: \n",
    "        question\t\n",
    "        answer\t\n",
    "        answer_number\t\n",
    "        equation_solution\t\n",
    "        language\n",
    "    \"\"\"\n",
    "\n",
    "    return user_header + \"\\n\\n\" + user_prompts[language] + \"\\n\\n\" + question + \"<|eot_id|>\" + assistant_header + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Fine-tune model on MGSM training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create parser for the model's final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Gemma's GSM8k Evaluation Code: https://github.com/google-deepmind/gemma/blob/main/colabs/gsm8k_eval.ipynb\n",
    "\n",
    "def find_numbers(x: str) -> list[str]:\n",
    "  \"\"\"Finds all numbers in a string.\"\"\"\n",
    "  # Search for number, possibly negative (hyphen), with thousand separators\n",
    "  # (comma), and with a decimal point (period inbetween digits).\n",
    "  numbers = re.compile(\n",
    "      r'-?[\\d,]*\\.?\\d+',\n",
    "      re.MULTILINE | re.DOTALL | re.IGNORECASE,\n",
    "  ).findall(x)\n",
    "  return numbers\n",
    "\n",
    "\n",
    "def find_number(x: str,\n",
    "                answer_delimiter: str = 'Answer:') -> str:\n",
    "  \"\"\"Finds the most relevant number in a string.\"\"\"\n",
    "  # If model uses the answer delimiter, then select the first number following\n",
    "  # that format.\n",
    "  if answer_delimiter in x:\n",
    "    answer = x.split(answer_delimiter)[-1]\n",
    "    numbers = find_numbers(answer)\n",
    "    # print(\"Found delimiter!\")\n",
    "    # print(numbers)\n",
    "\n",
    "    if numbers:\n",
    "      return numbers[0]\n",
    "\n",
    "  # In general, select the last number in the string.\n",
    "  numbers = find_numbers(x)\n",
    "  # print(numbers)\n",
    "\n",
    "  if numbers:\n",
    "    return numbers[-1]\n",
    "  return ''\n",
    "\n",
    "\n",
    "def maybe_remove_comma(x: str) -> str:\n",
    "  # Example: 5,600 -> 5600\n",
    "  return x.replace(',', '')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generate model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = {\n",
    "    'max_new_tokens': eval_config['max_gen_len'],\n",
    "    'num_return_sequences': eval_config['num_generations'],\n",
    "    'output_scores': eval_config['return_logprobs'],\n",
    "    'pad_token_id': tokenizer.eos_token_id,\n",
    "    'do_sample': False,  # since temperature, top_k, top_p are 0\n",
    "    'top_p': None,\n",
    "    'temperature': None,\n",
    "    'top_k': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the profit, first calculate the increased value of the house after repairs:\n",
      "\n",
      "$80,000 + ($50,000 x 1.5) = $80,000 + $75,000 = $155,000\n",
      "\n",
      "Then, subtract the original purchase price from the increased value to find the profit:\n",
      "\n",
      "$155,000 - $80,000 = $75,000\n",
      "\n",
      "Answer: $75,000\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample inference\n",
    "sample_idx = 2\n",
    "\n",
    "input_prompt = add_input_prompt(mgsm_test[sample_idx]['question'], mgsm_test[sample_idx]['language'])\n",
    "input_tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "output = model.generate(**input_tokenized, \n",
    "                        **generate_config\n",
    "                        )\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# splice output_text after \"assistant\"\n",
    "assistant_idx = output_text.find(\"assistant\\n\\n\")\n",
    "output_text = output_text[assistant_idx + len(\"assistant\\n\\n\"):]\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Answer: 75000\n"
     ]
    }
   ],
   "source": [
    "answer_delimiter = answer_translations[mgsm_test[sample_idx]['language']]\n",
    "output_numerical_answer = maybe_remove_comma(find_number(output_text, answer_delimiter))\n",
    "\n",
    "print(\"Numerical Answer:\", output_numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/110 [18:55<11:14:55, 378.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[286], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Generate outputs from the model for the entire batch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Get numerical answer from output texts\u001b[39;00m\n\u001b[1;32m     35\u001b[0m output_texts \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/generation/utils.py:3008\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3005\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3007\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    990\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m         position_embeddings,\n\u001b[1;32m    998\u001b[0m     )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:729\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:667\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    665\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 667\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai231-venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters for batch processing\n",
    "loader_config = {'batch_size': 25,\n",
    "                 'num_workers': 4, \n",
    "                 'prefetch_factor': 2,\n",
    "                 'shuffle': False}\n",
    "\n",
    "# mgsm_test = mgsm_test.remove_columns([\"answer\", \"equation_solution\"])\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "test_data_loader = DataLoader(mgsm_test, **loader_config)\n",
    "\n",
    "# Initializa a dictionary to store the results\n",
    "results_mgsm = {'question': mgsm_test['question'], \n",
    "                'language': mgsm_test['language'], \n",
    "                'ground_truth': mgsm_test['answer_number'], \n",
    "                'generated_answer': []}\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_samples in tqdm(test_data_loader, total=len(test_data_loader)):\n",
    "\n",
    "    # Add input prompt to the batch problems\n",
    "    batch_problems = [add_input_prompt(question, language) for question, language in zip(batch_samples['question'], batch_samples['language'])]\n",
    "    \n",
    "    # Tokenize the input problems\n",
    "    inputs = tokenizer(batch_problems, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate outputs from the model for the entire batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generate_config)\n",
    "\n",
    "    # Get numerical answer from output texts\n",
    "    output_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    numerical_answers = [maybe_remove_comma(find_number(output_text, answer_translations[language])) for output_text, language in zip(output_texts, batch_samples['language'])]\n",
    "\n",
    "    # Add the numerical answers to the results dictionary\n",
    "    results_mgsm['generated_answer'].extend(numerical_answers)\n",
    "\n",
    "# Create a directory to store the results\n",
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the results to a file\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(globals()[results_var_name], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mgsm['question'] = results_mgsm['question'][:75]\n",
    "results_mgsm['ground_truth'] = results_mgsm['ground_truth'][:75]\n",
    "results_mgsm['language'] = results_mgsm['language'][:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(globals()[results_var_name], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate score\n",
    "\n",
    "We calculate the average exact match (maj@1) scores over all eleven languages in the MGSM dataset. \\\n",
    "Reference: https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/eval_details.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per language: {'en': 0.32}\n",
      "Overall Accuracy: 0.32\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"outputs\"\n",
    "results_var_name = \"results_mgsm\"\n",
    "\n",
    "# Load the results from the json file and convert to pandas DataFrame\n",
    "results_file = os.path.join(output_dir, f\"{results_var_name}.json\")\n",
    "results_df = pd.read_json(results_file)\n",
    "\n",
    "# Convert ground_truth and generated_answer columns to float\n",
    "results_df['ground_truth'] = results_df['ground_truth'].astype(float)\n",
    "results_df['generated_answer'] = results_df['generated_answer'].astype(float)\n",
    "\n",
    "# Compare the generated answers with the ground truth answers\n",
    "results_df['correct'] = results_df['ground_truth'] == results_df['generated_answer']\n",
    "\n",
    "# Get accuracy per language then get average accuracy\n",
    "accuracy_per_language = results_df.groupby('language')['correct'].mean()\n",
    "overall_accuracy = accuracy_per_language.mean()\n",
    "\n",
    "print(\"Accuracy per language:\", accuracy_per_language.to_dict())\n",
    "print(\"Overall Accuracy:\", overall_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
